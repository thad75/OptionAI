{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thad75/OptionAI/blob/main/TP3/TP3_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk7CUTZFN4uW"
   },
   "source": [
    "# Lab 3, Part 1 : N_Gram on WikiText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbsta7lIHhz"
   },
   "source": [
    "## Download WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfGVtATnIHh2"
   },
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW7UOlBNIHh5"
   },
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5edQma8IHh7"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iijp5LYrORSL"
   },
   "source": [
    "# About this lab\n",
    "\n",
    "In this notebook, you will work with the N-Gram Language Model on the Wiki Text Dataset. This first lab should take you approximatively 1h.\n",
    "\n",
    "<div align =\"center\"><img src=\"https://www.w3.org/TR/ngram-spec/tree2.gif\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvjT2fTlIHh9"
   },
   "source": [
    "# A - The N-Gram Language Model\n",
    "\n",
    "A language model assigns a probability to each possible next word given a history of previous words (context). \n",
    "\n",
    "<div align='center'> $P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$ </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77l_VuoMPhzO"
   },
   "source": [
    "\n",
    "## Markov Assumption\n",
    "\n",
    "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
    "\n",
    "It assumes that each next word only depends on the previous K words (in an N-Gram language model, K = N-1).\n",
    "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
    "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
    "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
    "\n",
    "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
    "\n",
    "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$\n",
    "\n",
    "\n",
    "We provide you the code of the NGram Language Model.\n",
    "\n",
    "* Understand the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ql01Z8IiIHh_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NGramLM():\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.vocab = set()\n",
    "        self.data = []\n",
    "        self.prob = {}\n",
    "        self.counts = defaultdict(Counter)\n",
    "    \n",
    "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
    "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
    "    def train(self, vocab, data, smoothing_k=0):\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.smoothing_k = smoothing_k\n",
    "\n",
    "        if self.N == 1:\n",
    "            self.counts = Counter(flatten(data))\n",
    "            self.prob = self.get_prob(self.counts)\n",
    "        else:\n",
    "            self.vocab.add('<s>')\n",
    "            counts = self.count_ngram()\n",
    "            \n",
    "            self.prob = {}\n",
    "            for context, counter in counts.items():\n",
    "                self.prob[context] = self.get_prob(counter)\n",
    "    \n",
    "    def count_ngram(self):\n",
    "        counts = defaultdict(Counter)\n",
    "        for sentence in self.data:\n",
    "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
    "            for i in range(len(sentence)-self.N+1):\n",
    "                context = sentence[i:i+self.N-1]\n",
    "                context = \" \".join(context)\n",
    "    \n",
    "    # For N = 1, the probability is stored in a dict   P = prob[next_word\n",
    "                word = sentence[i+self.N-1]\n",
    "                counts[context][word] += 1\n",
    "\n",
    "        self.counts = counts\n",
    "        return counts\n",
    "        \n",
    "    # normalize counts into probability(considering smoothing)\n",
    "    def get_prob(self, counter):\n",
    "        total = float(sum(counter.values()))\n",
    "        k = self.smoothing_k\n",
    "    \n",
    "    # For N = 1, the probability is stored in a dict   P = prob[next_word\n",
    "        \n",
    "        prob = {}\n",
    "        for word, count in counter.items():\n",
    "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
    "        return prob\n",
    "        \n",
    "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
    "        if self.N == 1 and word in self.prob.keys():\n",
    "            return math.log(self.prob[word]) / seq_len\n",
    "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
    "            return math.log(self.prob[context][word]) / seq_len\n",
    "        else:\n",
    "            # assign a small probability to the unseen ngram\n",
    "            # to avoid log of zero and to penalise unseen word or context\n",
    "            return math.log(1/len(self.vocab)) / seq_len\n",
    "        \n",
    "    def get_ngram_prob(self, word, context=\"\"):\n",
    "        if self.N == 1 and word in self.prob.keys():\n",
    "            return self.prob[word]\n",
    "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
    "            return self.prob[context][word]\n",
    "        elif word in self.vocab and self.smoothing_k > 0:\n",
    "            # probability assigned by smoothing\n",
    "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
    "        else:\n",
    "            # unseen word or context\n",
    "            return 0\n",
    "            \n",
    "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
    "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
    "    def perplexity(self, test_data):\n",
    "        log_ppl = 0\n",
    "        if self.N == 1:\n",
    "            for sentence in test_data:\n",
    "                for word in sentence:\n",
    "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
    "        else:\n",
    "            for sentence in test_data:\n",
    "                for i in range(len(sentence)-self.N+1):\n",
    "                    context = sentence[i:i+self.N-1]\n",
    "                    context = \" \".join(context)\n",
    "                    word = sentence[i+self.N-1]\n",
    "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
    "        log_ppl /= len(test_data)\n",
    "        ppl = math.exp(-log_ppl)\n",
    "        return ppl\n",
    "    \n",
    "    def _is_unseen_ngram(self, context, word):\n",
    "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # generate the most probable k words\n",
    "    def generate_next(self, context, k):\n",
    "        context = (self.N-1) * '<s> ' + context\n",
    "        context = context.split()\n",
    "        ngram_context_list = context[-self.N+1:]\n",
    "        ngram_context = \" \".join(ngram_context_list)\n",
    "        \n",
    "        if ngram_context in self.prob.keys():\n",
    "            candidates = self.prob[ngram_context]\n",
    "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            for i in range(min(k, len(most_probable_words))):\n",
    "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
    "        else:\n",
    "            print(\"Unseen context!\")\n",
    "            \n",
    "    # generate the next n words with greedy search\n",
    "    def generate_next_n(self, context, n):\n",
    "        context = (self.N-1) * '<s> ' + context\n",
    "        context = context.split()\n",
    "        ngram_context_list = context[-self.N+1:]\n",
    "        ngram_context = \" \".join(ngram_context_list)\n",
    "        \n",
    "        for i in range(n):\n",
    "            try:\n",
    "                candidates = self.prob[ngram_context]\n",
    "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
    "                context += [most_likely_next]\n",
    "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
    "                ngram_context = \" \".join(ngram_context_list)\n",
    "            except:\n",
    "                break\n",
    "        print(\" \".join(context[self.N-1:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VwN9CXyIHiC"
   },
   "source": [
    "# B -Train with toy dataset\n",
    "\n",
    "As the model is created we can train a N-Gram LM. At this step, let's train a Bigram language model on the toy dataset.\n",
    "\n",
    "* What preprocessing was done ?\n",
    "* How many word do we have in our vocabulary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sabDGGntIHiC"
   },
   "outputs": [],
   "source": [
    "corpus = [\"I like whipped cream\",\n",
    "         \"I like candys\",\n",
    "         \"I hate tomatoes\"]\n",
    "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
    "vocab = set(flatten(data))\n",
    "\n",
    "# TODO : Answer the questions\n",
    "print(data)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HKqP6zvIHiE"
   },
   "outputs": [],
   "source": [
    "def print_probability(lm):\n",
    "    for context in lm.vocab:\n",
    "        for word in lm.vocab:\n",
    "            prob = lm.get_ngram_prob(word, context)\n",
    "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y-jHF6bIHiF"
   },
   "source": [
    "# C - Smoothing\n",
    "Smoothing method is used to deal with the sparsity problem in N-Gram language modelling.\n",
    "The probability mass is shifted towards the less frequent words.\n",
    "\n",
    "For example, with an add-1 smoothing, the probability is calculated as:\n",
    "\n",
    "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQB4IJYPQei1"
   },
   "source": [
    "**Q1: What is the disadvantage of smoothing?**\n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBS-MIWiIHiG"
   },
   "outputs": [],
   "source": [
    "lm = NGramLM(2)\n",
    "lm.train(vocab, data, smoothing_k=0)\n",
    "\n",
    "########################################################\n",
    "# TODO: try with add-2 smoothing and see the probability\n",
    "########################################################\n",
    "lm.train(vocab, data, smoothing_k=...)\n",
    "\n",
    "print_probability(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG35kkhVIHiH"
   },
   "source": [
    "# D -Train on WikiText-2 dataset and Evaluate Perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2DYmW1UQ5hF"
   },
   "source": [
    "## Evaluating perplexity\n",
    "\n",
    "We define the perplexity as \n",
    "\n",
    "<div align=\"center\">\n",
    "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
    "\n",
    "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G5C7IV_RLzi"
   },
   "source": [
    "\n",
    "\n",
    "**Q2: Why do we need to calculate log perplexity?**\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oYHXrWqRYLP"
   },
   "outputs": [],
   "source": [
    "vocab, train_data = prepare_data('train.txt')\n",
    "_, valid_data = prepare_data('valid.txt')\n",
    "_, test_data = prepare_data('test.txt')\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYcGOj6cIHiI"
   },
   "outputs": [],
   "source": [
    "lm = NGramLM(3)\n",
    "\n",
    "# Train the model using vocab and training data\n",
    "\n",
    "lm....(...,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh0NmO16IHiI"
   },
   "outputs": [],
   "source": [
    "print(lm.perplexity(valid_data))\n",
    "print(lm.perplexity(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw9EMl6XIHiK"
   },
   "source": [
    "# E - Generating sentences\n",
    "\n",
    "With a pre-trained N-Gram language model, we can predict possible next words given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVE1LyfxIHiL"
   },
   "outputs": [],
   "source": [
    "# generate the most probable following words given the context\n",
    "print(\" \".join(valid_data[12]))\n",
    "\n",
    "# actually the only useful context in the Trigram language model is [\"where\", \"they\"]\n",
    "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
    "lm.generate_next(context, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1y3MW9DZ-nb"
   },
   "outputs": [],
   "source": [
    "# we can also generate with shorter contexts, even shorter than N-1\n",
    "\n",
    "contexts = [\"the eggs\",\n",
    "            \"the\",\n",
    "            \"\"]\n",
    "for context in contexts:\n",
    "  lm.generate_next(context, 3)\n",
    "  print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIiRSWPoIHiM"
   },
   "outputs": [],
   "source": [
    "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
    "\n",
    "# generate the next n words with greedy search\n",
    "lm.generate_next_n(context, 10)\n",
    "\n",
    "# This is not a good method in practice,\n",
    "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
    "lm.generate_next_n(context, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSEVmuouIHiN"
   },
   "source": [
    "# F - Effect of N\n",
    "\n",
    "**Q3: Why does the perplexity increase when N is large?**\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLVeKE_JIHiN"
   },
   "outputs": [],
   "source": [
    "for n in range(1,6):\n",
    "    lm = NGramLM(n)\n",
    "    lm.train(vocab, train_data)\n",
    "    print(\"************************\")\n",
    "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
    "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgM5mePhIHiO"
   },
   "source": [
    "# G - Interpolation\n",
    "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f57uTZvIHiO"
   },
   "outputs": [],
   "source": [
    "class InterpolateNGramLM(NGramLM):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super(InterpolateNGramLM, self).__init__(N)\n",
    "        self.ngram_lms = []\n",
    "        self.lambdas = []\n",
    "        \n",
    "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
    "        assert len(lambdas) == self.N\n",
    "        assert sum(lambdas) - 1 < 1e-9\n",
    "        self.vocab = vocab\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        for i in range(self.N, 0, -1):\n",
    "            # Instanciate the model\n",
    "            lm = ...\n",
    "            print(\"Training {}-gram language model\".format(i))\n",
    "            lm.train(vocab, data, smoothing_k)\n",
    "            self.ngram_lms.append(lm)\n",
    "    \n",
    "    def get_ngram_logprob(self, word, seq_len, context):\n",
    "        prob = 0\n",
    "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
    "            context_words = context.split()\n",
    "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
    "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
    "        return math.log(prob) / seq_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TafZ7KpIHiO"
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Q5: tune your coefficients to decrease perplexity\n",
    "###################################################\n",
    "ilm = InterpolateNGramLM(3)\n",
    "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZGf_gQOIHiP"
   },
   "outputs": [],
   "source": [
    "print(ilm.perplexity(valid_data))\n",
    "print(ilm.perplexity(test_data))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
