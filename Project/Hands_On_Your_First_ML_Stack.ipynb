{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/OptionAI/blob/main/Project/Hands_On_Your_First_ML_Stack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mbB7j6SK0pa"
      },
      "source": [
        "Disclaimer : You will be accessing lots of free frameworks using your Google Account. Feel free to revoke them after the end of the Option AI Classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfVtHtyLLBEA"
      },
      "source": [
        "# Hands-On : Building your first simple stack. (3h)\n",
        "\n",
        "<img src=\"https://cdn-lfs.huggingface.co/repos/96/a2/96a2c8468c1546e660ac2609e49404b8588fcf5a748761fa72c154b2836b4c83/9cf16f4f32604eaf76dabbdf47701eea5a768ebcc7296acc1d1758181f71db73?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27hf-logo.png%3B+filename%3D%22hf-logo.png%22%3B&response-content-type=image%2Fpng&Expires=1710601193&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDYwMTE5M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85Ni9hMi85NmEyYzg0NjhjMTU0NmU2NjBhYzI2MDllNDk0MDRiODU4OGZjZjVhNzQ4NzYxZmE3MmMxNTRiMjgzNmI0YzgzLzljZjE2ZjRmMzI2MDRlYWY3NmRhYmJkZjQ3NzAxZWVhNWE3NjhlYmNjNzI5NmFjYzFkMTc1ODE4MWY3MWRiNzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=TczHubMkO7sMQxJKNGcmSmf-sZS%7E83h7X6duKusZrR8cra12wUABImnMsH8YM1tVSAZll76Pv99baOONGMH4RhaLBAlwIyzWX8erIyamju1Jt0Gf%7EP1dQzJ6UpxR-yjHdIPiHM2DSBgHUnUsgBnO11Xrk2e0qN1wMvApi4vnx0yaiefGMal46AWmn8-oxA5QecoI3-PobGX-IFunsTbecKo9WCfgsezeA4HwGXENhlG%7E79feGAz3KO2TdJBm0pOSJnB4Gxu5RIERj1%7E4w1-RBnEA12xDpvYwNztyWrw05gfk6dZ3FwEqVtd97GmTJtXNF38fYmfINJwYK8wfa82JtA__&Key-Pair-Id=KVTP0A1DKRTAX\"  width=\"200\" height=\"200\">\n",
        "\n",
        "<img src='https://pypi-camo.freetls.fastly.net/a95ef5913dc4cc84d2155ff690a0fa0d4c33d7e2/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f67726164696f2d6170702f67726164696f2f6d61696e2f726561646d655f66696c65732f67726164696f2e737667'  width=\"200\" height=\"200\">\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/wandb/assets/main/wandb-logo-yellow-dots-black-wb.svg\"  width=\"200\" height=\"200\">\n",
        "\n",
        "\n",
        "Welcome to the world of AI engineering! In this course, we're going to build a stack for training a summarization model. Don't worry if you don't grasp every minute detail of the frameworks introduced – our main objectives are to:\n",
        "\n",
        "1. Get acquainted with future MLOps practices.\n",
        "2. Engage in hands-on AI practice, constructing a partially functional stack.\n",
        "3. Create something meaningful that you can proudly include on your resume.\n",
        "4. Open the door to internship opportunities.\n",
        "\n",
        "Let's embark on this journey together and lay the foundation for your AI engineering expertise.\n",
        "\n",
        "**You will encounter lots of issues in this lab. And it is completely normal. Feel free to think A LOT before asking any questions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-507CL8QGVHK"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "!pip install wandb -qU\n",
        "!pip install transformers datasets evaluate rouge_score\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ci101UhGYDg"
      },
      "source": [
        "# I - Data Layer\n",
        "\n",
        "## Load and Explore\n",
        "\n",
        "\n",
        "As always, data forms the foundation of our ML systems. The principle of GIGO (Garbage in, garbage out) emphasizes that the quality of input data significantly impacts the performance of your production system.\n",
        "\n",
        "We have a basic dataset for subsequent parts, starting with the data layer. Let's delve into the cnn_dailymail dataset, exploring and analyzing its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndwK98IEgWpT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0', split=['train',\n",
        "                                                        'validation',\n",
        "                                                        'test[:1%]'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF8DHoJjvGKZ"
      },
      "outputs": [],
      "source": [
        "# TODO : What format is dataset ?\n",
        "# TODO : Does it contain everything we need for a good training ?\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqYT_3JJvGBz"
      },
      "outputs": [],
      "source": [
        "# TODO : Pick the sample at index 0 from the dataset train. What keys are present ? What do they characterize ?\n",
        "dataset..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfpBuJJsIWkc"
      },
      "outputs": [],
      "source": [
        "# TODO : Pick the sample at index 0 from the dataset valid. What keys are present ? What do they characterize ?\n",
        "dataset..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ1kKUY9IWrm"
      },
      "outputs": [],
      "source": [
        "# TODO : Pick the sample at index 0 from the dataset test. What keys are present ? What do they characterize ?\n",
        "dataset..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH9KAT-6GY-G"
      },
      "source": [
        "# II - Data to Model\n",
        "\n",
        "Now, let's talk about the key tools we need: a **model** and its sidekick. In the NLP world, the trusty sidekick is called a **tokenizer**, and it's pretty much the model's best friend.\n",
        "We will leverage from the Seq2Seq model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYqNK30VJm6s"
      },
      "source": [
        "### a - Tokenizer and Preprocessing\n",
        "\n",
        "Humans understand natural language. However a model does not. As mentionned in the class, we represent words or subwords as tokens. Tokens are tensors that gave a representation within a space. Each pretrained model has its own representation space, based on the training vocab.\n",
        "We are going to translate the whole dataset into tokens.\n",
        "This process will take around 10 min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF5FfZURgWDT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvTHIic2Qod0"
      },
      "source": [
        "Once the tokenizer loaded, we can either have to remap the training dataset to tokens. Obviously, there are functions that does that. In this case, we have to tokenize the input and also the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5b7536f84a474d61ad7172e5483d6b40",
            "47716c4a76b942c8a7cefbebad1cbd2c",
            "7398f9702ae5478a952c3a41b0f173fd",
            "9470852514de42279b47b3b09bfad15a",
            "dceaf3f21ffd4b748ea42d1a772b9c25",
            "77c4bebc334f4ebcb43c7baab89df337",
            "eb6381bfabb84be98bc4767d2bb8ccdb",
            "6e9da9383c1241b5a17d64d2e3b6755b",
            "df3befbf47fc4e90b3b8fbc8c2593ee5",
            "1361ba6d64d34d0db704f7ee68f7def0",
            "91de5fa148c346a49cb2be32a45fbf3e"
          ]
        },
        "id": "jAmFrUN_vjI8",
        "outputId": "df256c25-f79d-49c6-a561-6a0c8d665443"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b7536f84a474d61ad7172e5483d6b40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/134 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=1024,\n",
        "                             truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"highlights\"],\n",
        "                       max_length=128,\n",
        "                       truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# TODO : Map your datasets using your tokenizers.\n",
        "tokenized_train = dataset[...].map(preprocess_function, batched=True)\n",
        "tokenized_valid = dataset[...].map(preprocess_function, batched=True)\n",
        "tokenized_test = dataset[...].map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BASDzftm5WT"
      },
      "source": [
        "After preparing the dataset, you can set up a datacollator, which essentially works like a dataloader. It organizes the data into batches and can also apply specific random changes if needed. Depending on your project, you might opt to craft custom Dataloaders. In our situation, we'll stick with the pre-existing one provided by HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRF-a-dsvrMB"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
        "                                       model=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9t3WPRDRMX"
      },
      "source": [
        "### b - Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwAr__b6PgOs"
      },
      "source": [
        "To assess NLP models for summarization, ROUGE metrics are commonly used. ROUGE, which stands for Recall-Oriented Understudy for Gisting Evaluation, compares automatically generated summaries or translations to human-produced reference summaries or translations.\n",
        "\n",
        "ROUGE metrics provide scores between 0 and 1, where a higher score indicates greater similarity between the automatically generated summary and the reference summary.\n",
        "\n",
        "Rather than reinventing the wheel, let's leverage the existing metric computation suite. It's likely that another engineer has already done the work. Our approach will involve building upon the existing solutions rather than starting from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceDFCApDvtn1"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "# TODO : Load the rouge evaluation\n",
        "rouge = evaluate.load(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmwFSFKlobG_"
      },
      "source": [
        "To integrate the evaluation pipeline into our Model Layer, we'll need to customize it to suit our specific needs. This personalization ensures that the evaluation process aligns seamlessly with the requirements of our model and enhances its usability within the broader Model Layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6FbacmZwCuv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-2sOifDWM3"
      },
      "source": [
        "### c - Loading Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkoX2DgMuEIA"
      },
      "source": [
        "Let's efficiently load the model from the pre-trained checkpoint to capitalize on the knowledge acquired during pre-training. This step allows us to build upon the expertise encoded in the pre-existing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pskon21wFDj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfo96r39DesA"
      },
      "source": [
        "### d - Train the model you must"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLH2FR6Dut2s"
      },
      "source": [
        "Becoming an AI engineer is where the fun begins! Models often respond differently to various hyperparameters. To explore and find the best configuration, we typically prepare multiple scripts with different hyperparameters. These configurations are stored in structured data formats like .YAML or .JSON files. HuggingFace provides a Seq2Seq training argument class that we can utilize to set up and customize our model with ease. This allows us to experiment with hyperparameters and optimize the model's performance.\n",
        "\n",
        "Previously, for logging, we utilized Tensorboard, a free monitoring tool, but it had numerous bugs. Companies often turn to alternatives like W&B and MLFlow for model monitoring. In this lab, we'll be using W&B, a sleek dashboard. The cool part with HuggingFace is that it automatically manages the monitoring module by checking your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38aM9g6LwHgO"
      },
      "outputs": [],
      "source": [
        "# TODO : Fill the arguments using your knowledge\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"....\", # TODO : Give an output directory name for you project.\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate= ...,    # TODO : Setup a learning rate. Remember that we are fine-tuning.\n",
        "    per_device_train_batch_size=...,\n",
        "    per_device_eval_batch_size=...,\n",
        "    weight_decay=....,\n",
        "    save_total_limit=...,\n",
        "    num_train_epochs=...,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False, # If you have a HF Account and want to push your trained model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3EXKhqSvhLg"
      },
      "source": [
        "It's time to create a Trainer – the class that will take charge of the training process for us. This class streamlines the training workflow, making the entire process more manageable and efficient.\n",
        "\n",
        "The model should take 10 min to train. If you have any questions feel free to ask them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-NUrZ3bvgnx"
      },
      "outputs": [],
      "source": [
        "# TODO : Initialize the model with the correct arguments\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model= ...,\n",
        "    args= ...,\n",
        "    train_dataset= ...,\n",
        "    eval_dataset= ...,\n",
        "    tokenizer= ...,\n",
        "    data_collator= data_collator,\n",
        "    compute_metrics= compute_metrics,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kkqDVptGZsL"
      },
      "source": [
        "# III - Serving\n",
        "\n",
        "\n",
        "Now that we can assess that our model works, we have to deploy it to the end user. Typically, someone that is going to use the model but on an interface like what ChatGPT,Bard does. Deploiement could also be done on hardware materials. But we won't go into that field.\n",
        "\n",
        "In this way, we will be leveraging from Gradio. Gradio serves as a simple tool to quickly develop a interface for serving. However, in reality there's lot more going behind. In companies, we create API with separate back and front ends to fullfil the need of the client.\n",
        "\n",
        "Let's plug our model to gradio and test the front end interface given to us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_hCV44GXEld"
      },
      "outputs": [],
      "source": [
        "from gradio import Interface\n",
        "\n",
        "name = '...'\n",
        "\n",
        "def summarize(text, sumup = True):\n",
        "    # TODO : Fill the missing stuff from the text pipeline\n",
        "    input_ids = tokenizer(...,\n",
        "                          return_tensors=\"pt\")[\"input_ids\"]\n",
        "    output = model.generate(input_ids.to(model.device))\n",
        "    summary = tokenizer.decode(output[0],\n",
        "                               skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Create the Gradio interface\n",
        "interface = Interface(\n",
        "    fn=summarize,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=f\"{name}'s first ML Stack Deployed\",\n",
        "    description=\"Enter text to get a summary using your seq2seq model.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch(debug = True,\n",
        "                 share = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEF3aXG47vf3"
      },
      "source": [
        "If you notice problems with your model's predictions, there's a reason for it. We encourage you to investigate the cause behind these issues. Now, let's get back to the class."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1361ba6d64d34d0db704f7ee68f7def0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47716c4a76b942c8a7cefbebad1cbd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77c4bebc334f4ebcb43c7baab89df337",
            "placeholder": "​",
            "style": "IPY_MODEL_eb6381bfabb84be98bc4767d2bb8ccdb",
            "value": "Map: 100%"
          }
        },
        "5b7536f84a474d61ad7172e5483d6b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47716c4a76b942c8a7cefbebad1cbd2c",
              "IPY_MODEL_7398f9702ae5478a952c3a41b0f173fd",
              "IPY_MODEL_9470852514de42279b47b3b09bfad15a"
            ],
            "layout": "IPY_MODEL_dceaf3f21ffd4b748ea42d1a772b9c25"
          }
        },
        "6e9da9383c1241b5a17d64d2e3b6755b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7398f9702ae5478a952c3a41b0f173fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e9da9383c1241b5a17d64d2e3b6755b",
            "max": 134,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df3befbf47fc4e90b3b8fbc8c2593ee5",
            "value": 134
          }
        },
        "77c4bebc334f4ebcb43c7baab89df337": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91de5fa148c346a49cb2be32a45fbf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9470852514de42279b47b3b09bfad15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1361ba6d64d34d0db704f7ee68f7def0",
            "placeholder": "​",
            "style": "IPY_MODEL_91de5fa148c346a49cb2be32a45fbf3e",
            "value": " 134/134 [00:01&lt;00:00, 96.96 examples/s]"
          }
        },
        "dceaf3f21ffd4b748ea42d1a772b9c25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3befbf47fc4e90b3b8fbc8c2593ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb6381bfabb84be98bc4767d2bb8ccdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}