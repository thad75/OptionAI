{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/OptionAI/blob/main/TP%202%20Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2 : Sentiment Analysis"
      ],
      "metadata": {
        "id": "acBNgXp5Dn1I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEJM0h_uctzV"
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# we fix the seeds to get consistent results before every training\n",
        "# loop in what follows\n",
        "def fix_seed(seed=234):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8B9-L8U0aZ"
      },
      "source": [
        "## About this lab\n",
        "\n",
        "In this notebook we are going to build state-of-the art models for text classification using the example of sentiment analysis. To be more precise, we will build a feed-forward neural network (FFNN) and a convolutional neural network (CNN). We will look into the details of data preparation, functioning of each model and how the performance of those NNs could be measured efficiently. We will start our work using a toy corpus. Further you can extend your knowledge and use a larger dataset.\n",
        "\n",
        "Again we are using [pytorch](https://www.pytorch.org), an open source deep learning platform, as our backbone library in the course.\n",
        "\n",
        "Goal of this lab : \n",
        "* Understand word representation\n",
        "* Practice your Deep Learning Skills\n",
        "* Train a model for NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toying around\n",
        "\n",
        "We present to you our toy datasets : the toy training and validation sets. It is good practice to use the validation set (a representative set of the test data). This set is used to tune hyperparameters and choose a configuration for your model to ensure the best performance. \n",
        "\n",
        "Our toy sets are already tokenized and lowercased."
      ],
      "metadata": {
        "id": "DA20HHlTEH1E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abMSnMwJx8bu"
      },
      "source": [
        " # Our toy sentiment analysis corpus\n",
        "train = ['i like his new book !',\n",
        "         'what a well-written novel !',\n",
        "         'i do not agree with the criticism on this short story',\n",
        "         'well done ! it was an enjoyable stage play',\n",
        "         'it was very good . send me a copy please .',\n",
        "         'the argumentation in the study is very weak',\n",
        "         'poor effort !',\n",
        "         'the descriptions could have been more detailed',\n",
        "         'i am not impressed',\n",
        "         'could have done better .',\n",
        "]\n",
        "\n",
        "train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "# Validation set\n",
        "valid = ['i like your play', \n",
        "         'i agree with your study', \n",
        "         'what a success ! a well-written novel', \n",
        "         'not enough details . very poor', \n",
        "         'i support the criticism',\n",
        "         'could be better',\n",
        "]\n",
        "\n",
        "valid_labels = [1, 1, 1, 0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPlpqVFYpfL"
      },
      "source": [
        "### Pre-processing\n",
        "\n",
        "* Using simple list operators, fill in the below function '...' to tokenize the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ5IL1e1GVn6"
      },
      "source": [
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  #######################\n",
        "  # Q: Process the corpus\n",
        "  #######################\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = \n",
        "    for token in sentence.split(' '): \n",
        "      tokenized_sentence....(token)\n",
        "    tokenized_corpus....(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKCfY6QbO0r"
      },
      "source": [
        "### Word2index dictionary\n",
        "\n",
        "In order to easily retrieve each word in the vocabulary, we define here a method that returns a word to index dictionary. Note that we reserve the 0 index for the padding token `<pad>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bWQ3zKYKlQU"
      },
      "source": [
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the padding token\n",
        "  word2idx['<pad>'] = 0\n",
        "  \n",
        " \n",
        "  return word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hve1MProNBvp"
      },
      "source": [
        "### Preparation of inputs\n",
        "\n",
        "The first layer of our FFNN will be an embedding (look-up) layer which takes as input indexes of tokens (we do not need to one-hot encode our vectors).\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why do we need to fix the length of our input vectors (we take the maximum sentence length here) ? This process is referred to as padding. Print the padded training corpus.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-xp_gKvObfw"
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "\n",
        "  # Get maximum length\n",
        "  max_len = max(...)\n",
        "  \n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(...)\n",
        "  \n",
        "  return sent_tensor, label_tensor\n",
        "\n",
        "###\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train)\n",
        "word2idx = get_word2idx(...)\n",
        "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGg6CtALe8Va"
      },
      "source": [
        "## Model 1 : the Feed-Forward Neural Network\n",
        "\n",
        "We will start by building a very simple feed-forward neural network (FFNN).\n",
        "Our FFNN class is a sub-class of `nn.Module`. Within the `__init__` method, we define the layers of the module:\n",
        "\n",
        "- Our first layer is an embedding layer (look-up layer). This layer could be initialized with pre-trained embeddings (as we will see at the end of this lab) or could be trained together with other layers.\n",
        " \n",
        "- The next layer is a fully connected layer followed by a ReLU activation.\n",
        "\n",
        "- Finally, the last linear layer is the output layer for the classification task.\n",
        "\n",
        "The `forward()` method is called when we feed data into our model. Please note that the output dimension of each layer is the input dimension for the next one.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Q: Implement the averaging of embeddings in the `forward()` method of the class below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LWut1gtXGQN"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n",
        "        super(FFNN, self).__init__()\n",
        "        \n",
        "        # embedding (lookup layer) layer\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        \n",
        "        # hidden layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        \n",
        "        # activation\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # output layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, max_sent_len)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        # `embedding` has shape (batch size, max_sent_len, embedding dim)\n",
        "\n",
        "        ########################################################################\n",
        "        # Q: Compute the average embeddings of shape (batch_size, embedding_dim)\n",
        "        ########################################################################\n",
        "        # Implement averaging that ignores padding (average using actual sentence lengths).\n",
        "        # How this effect the result?\n",
        "        \n",
        "        sent_lens = ...\n",
        "        averaged = ... / sent_lens\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAI5BtaQMFh"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "In this section we will define the hyperparameters of our model, the loss function, the optimizer and perform a number of training epochs over our toy training corpus.\n",
        "\n",
        "We will use the **Stochastic gradient descent (SGD)** optimizer. The learning rate hyperparameter of the optimizer controls how the weights are adjusted with respect to the loss gradient. The lower the value, the more fine-grained are weight updates.\n",
        "\n",
        "**Note that** it is a common practise to perform training using mini-batches (sets of training instances seen by the model during weight update step). In this case, the epoch loss is defined as the loss averaged across the mini-batches. Since our corpus is very small, we train on the whole training set without batching.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why is the number of output classes is equal to 1 for binary classification?**\n",
        "\n",
        "\n",
        "**Q: Try to modify the learning rate (which is initially set to 0.5 below) in the range $[0.0001, 0.5]$. How does the loss react to these changes?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrtOwTUsyArb"
      },
      "source": [
        "# Reset the seed before every model construction for reproducible results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwY-k6x0xIkl"
      },
      "source": [
        "### Measuring the accuracy\n",
        "\n",
        "In addition to measuring the loss, we can also evaluate the actual classification performance of our model. (In the case of training with mini-batches, the epoch accuracy is defined as the accuracy averaged across the mini-batches.)\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Fill in the below function so that it computes the accuracy of the model. Once you are done, improve the loop so that it also prints the training accuracy after each epoch.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R23R0XjuPTn"
      },
      "source": [
        "def accuracy(output, target):\n",
        "  #####################################\n",
        "  # Q: Return the accuracy of the model\n",
        "  #####################################\n",
        "  acc = ...\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eP2s5MyyrNO"
      },
      "source": [
        "# Reset the seed for consistent results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "  \n",
        "  #####################\n",
        "  # Q: Compute accuracy\n",
        "  #####################\n",
        "  train_acc = ...\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2UhMGlMq3Re"
      },
      "source": [
        "### Hyperparameter tuning on the validation set\n",
        "\n",
        "You should now apply the previous pre-processing and input preparation procedures to the validation set as well.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Should we re-use the word to index dictionary we created before? Why?**\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9_FERooxUQR"
      },
      "source": [
        "###############################################\n",
        "# Q: Prepare the validation corpus and labels #\n",
        "###############################################\n",
        "print(valid_sent_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlXfydh1Uxv"
      },
      "source": [
        "**Q: Try to modify the learning rate and the number of epochs now. How will the validation loss and accuracy react to those changes?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcKPET9BHb-7"
      },
      "source": [
        "# Reset the seed for consistent results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors for training\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "# Input and label tensors for validation\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # Compute training accuracy\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
        "  # good practise to include even if we do not use them right now\n",
        "  model.eval()\n",
        "\n",
        "  # we do not compute gradients within this block, i.e. no training\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzWXeQATPs4"
      },
      "source": [
        "### Testing the model\n",
        "\n",
        "Now let us test our trained model. We define a small test set below. First, apply the data preparation procedures to this test set as you did for the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cx2eJi1a8R6"
      },
      "source": [
        "test = ['i really do not like your short story', \n",
        "        'well done', \n",
        "        'good results for a study !',\n",
        "        'amazing effort', \n",
        "        'your effort is poor !', \n",
        "        'not impressed'   \n",
        "]\n",
        "\n",
        "test_labels = [0, 1, 1, 1, 0, 0]\n",
        "\n",
        "#########################################\n",
        "# Q: Prepare the test corpus and labels #\n",
        "\n",
        "print(test_sent_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0A1Iu-p4FUL"
      },
      "source": [
        "**Q: Fill in the below function for the computation of F-measure. Once done, complete the missing lines in the final evaluation part.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixhvaYkW_SfX"
      },
      "source": [
        "def f_measure(output, gold):\n",
        "  ############################################\n",
        "  # Q: Compute precision, recall and f-measure \n",
        "  ############################################\n",
        "  pred = torch.round(torch.sigmoid(output))\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "     \n",
        "  test_pos_preds = np.sum(pred)\n",
        "  test_pos_real = np.sum(gold)\n",
        "    \n",
        "  correct = (np.logical_...pred, gold)).astype(int)\n",
        "  correct = np.sum(correct)\n",
        "  \n",
        "  precision = correct / test_pos_...\n",
        "  recall = correct / test_pos_...\n",
        "  \n",
        "  fscore = (2.0 * precision * recall) / (... + ...)\n",
        "\n",
        "  # Print them\n",
        "  print(f\"     Recall: {recall:.2f}, Precision: {precision:.2f}, F-measure: {fscore:.2f}\")\n",
        "  \n",
        "\n",
        "####\n",
        "\n",
        "model.eval()\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "  ####################################################################\n",
        "  # Q: Get predictions for the test set, compute the loss and accuracy\n",
        "  ####################################################################\n",
        "  predictions = ...\n",
        "  test_loss = ...\n",
        "  test_acc = ...\n",
        "\n",
        "  # Print\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "  f_measure(predictions, test_labels)  Recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMn0IjZ3lUr"
      },
      "source": [
        "**Q:  Are the resulting evaluations different ? How do you interpret those differences? Print the predictions.**\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2znwTJeU3UT"
      },
      "source": [
        "## Model 2 : the Convolutional Neural Network (CNN)\n",
        "\n",
        "We will implement a model inspired by the CNN model as described in [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882).\n",
        "\n",
        "Similar to the FFNN model, we start with an embedding layer. We implement the convolutional layer with the help of `nn.Conv2d` and use the ReLU activation after it. The above-mentioned paper, being inspired by the convolution for images, applies a 2-dimensional convolution: a (window size, embedding dimension) filter. It covers `n` sequential words, taking embedding dimensions as the width. We then pass the tensors through a **max pooling layer**.\n",
        "\n",
        "The **max pooling layer** is typically followed by a **dropout** layer. The latter sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Study the shapes of outputs coming from convolution and max pooling layers. What is the shape of the max pooling layer output?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUPETZaOgvgB"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    # Create the embedding layer as usual\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    \n",
        "    # in_channels -- 1 text channel\n",
        "    # out_channels -- the number of output channels\n",
        "    # kernel_size is (window size x embedding dim)\n",
        "    self.conv = nn.Conv2d(\n",
        "      in_channels=1, out_channels=out_channels,\n",
        "      kernel_size=(window_size, embedding_dim))\n",
        "    \n",
        "    # the dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # the output layer\n",
        "    self.fc = nn.Linear(out_channels, output_dim)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    # x -> (batch size, max_sent_length)\n",
        "    \n",
        "    embedded = self.embedding(x)\n",
        "    # embedded -> (batch size, max_sent_length, embedding_dim)\n",
        "    \n",
        "    # images have 3 RGB channels \n",
        "    # for the text we add 1 channel\n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    # embedded -> (batch size, 1, max_sent_length, embedding dim)\n",
        "\n",
        "    # Compute the feature maps      \n",
        "    feature_maps = self.conv(embedded)\n",
        "\n",
        "    ##########################################\n",
        "    # Q: What is the shape of `feature_maps` ?\n",
        "    ##########################################\n",
        "    \n",
        "    feature_maps = feature_maps.squeeze(3)\n",
        "    \n",
        "    ##########################################\n",
        "    # Q: why do we remove 1 dimension here?\n",
        "    ##########################################\n",
        "    \n",
        "    # Apply ReLU\n",
        "    feature_maps = F.relu(feature_maps)\n",
        "    \n",
        "    # Apply the max pooling layer\n",
        "    pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "    \n",
        "    pooled = pooled.squeeze(2)\n",
        "\n",
        "    ####################################\n",
        "    # Q: What is the shape of `pooled` ?\n",
        "    ####################################\n",
        "    \n",
        "    dropped = self.dropout(pooled)\n",
        "    preds = self.fc(dropped)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UZroMvLtFVv"
      },
      "source": [
        "### Training and testing the CNN\n",
        "\n",
        "Here we will define the CNN-specific hyperparameters and perform the network training and testing. **Note that** the learning rate is initially set to 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq1lcWbqRwNY"
      },
      "source": [
        "fix_seed()\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.1\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# the hyperparameters specific to CNN\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
        "\n",
        "\n",
        "## Finally, test on the test set\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(feature_test).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_test)\n",
        "    acc = accuracy(predictions, target_test)\n",
        "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
        "    f_measure(predictions, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5si3n_wf-ImU"
      },
      "source": [
        " **Q: Is the performance of CNN different from the performance of FFNN? Output predictions.**\n",
        " \n",
        "**Q: Is padding necessary for CNN inputs? What is the role of the window size?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4VCX4XSqvlu"
      },
      "source": [
        "### Initializing CNN with pre-trained representations\n",
        "\n",
        "The work [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882) also investigates the exploitation of pre-trained embeddings and demonstrates the efficiency of using them.\n",
        "\n",
        "First, download the embeddings and unzip them below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgvTCi68lOGn"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq8ou3Y0CSQh"
      },
      "source": [
        "# Unzip the file: 4 different embedding sizes are provided\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afclMKCYLYT9"
      },
      "source": [
        "# Check the file format\n",
        "!head -n10 glove.6B.50d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMwxUcgDHzGu"
      },
      "source": [
        "\n",
        "Try and initialize the CNN embedding layer with the `50D` pre-trained GloVe embeddings. Pay particular attention to keeping the correct indices from the `word2idx` for the lookup table! Once you fill the below `wvecs` matrix, copy the previous training loop and initialize its embedding layer with the pre-trained ones as follows:\n",
        "\n",
        "```python\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "```\n",
        "\n",
        "**Note:** The learning rate is initially set to 0.5.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: What should the embedding for the padding token `<pad>` be?**\n",
        " \n",
        "**Q: What is the impact of using those pre-trained embeddings on the model performance?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCsV8mtBg4-Y"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# Yet another hyperparameter: since the pre-trained embeddings are coming\n",
        "# from a different network, their magnitudes could differ from the parameters\n",
        "# of this network. So scaling may be necessary.\n",
        "SCALE_EMBS = 0.65\n",
        "\n",
        "# Creates the empty numpy array that you should fill below\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "#####################################################################\n",
        "# Q: Read line by line, find the corresponding word and\n",
        "# insert its embedding to the correct position in the `wvecs` matrix.\n",
        "# Once done, apply the SCALE_EMBS factor to scale the vectors\n",
        "#####################################################################\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
        "  \"<TODO>\"\n",
        "        \n",
        "print(wvecs)\n",
        "\n",
        "#####################\n",
        "# Re-create the model\n",
        "#####################\n",
        "fix_seed()\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.5\n",
        "\n",
        "# the hyperparameters specific to CNN\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.1\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "#################################################################\n",
        "### Q: Initialize the embeddings with the loaded pre-trained ones\n",
        "#################################################################\n",
        "...\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
        "\n",
        "\n",
        "## Finally, test on the test set\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(feature_test).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_test)\n",
        "    acc = accuracy(predictions, target_test)\n",
        "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
        "    f_measure(predictions, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}