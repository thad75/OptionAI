{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thad75/OptionAI/blob/main/TDm/SR/RecSysTD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puWxHBgcYLyM"
   },
   "outputs": [],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "!unzip \"/content/ml-20m.zip\"\n",
    "!pip install gdown\n",
    "!gdown --id 1b9XN-AD2ph6Akdy4gpWdx8rIlK6HZ-u9\n",
    "!pip install pytorch-lightning\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycocotools\n",
    "import pytorch_lightning as pl\n",
    "import scipy.sparse as sparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import clear_output, display\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQyytBeamtyk"
   },
   "source": [
    "# Introduction to the World of Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeUQGymaplI7"
   },
   "source": [
    "## Disclaimer \n",
    "\n",
    "These are interviews questions from companies such as PMU, Amazon, Netflix. Try not to use ChatGPT as much as possible as ChatGPT is not, yet, allowed in interviews.\n",
    "Don't hesitate to ask questions ! We are paid for that. A live correction will be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSaipFHbQE1A"
   },
   "source": [
    "## Goal of this lab\n",
    "\n",
    "\n",
    "*   Apply Recommendation Systems to Industrial Datasets\n",
    "*   Get to know Pytorch-Lightning\n",
    "*  **Be lost**\n",
    "* Discover new frameworks and read documentation\n",
    "* Explore Datas and think\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5mobf2WMiey"
   },
   "source": [
    "# Hands-On I : Recommending an artist\n",
    "\n",
    "Given the following data, we want to recommend a list of artists from our existing database to the listener. \n",
    "\n",
    "*  Using similarity computation, recommend in the decreasing order the artists \n",
    "that suits listerners' taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is2l__NFM6bb"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Artist': ['Celine Dion', 'Fred Again', 'Elton John', 'Black Eyed Peas', 'BTS', 'Anirudh', 'A.R Rahman', 'User'],\n",
    "    'International': [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'Sings in French': [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'Versatility': [0, 0, 0, 0, 0, 1, 1, 1],\n",
    "    'Danceability': [0, 1, 0, 1, 1, 1, 1, 0],\n",
    "    'Makes Film Music (BGM)': [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    'Won Awards': [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'Recent Hit': [0, 1, 0, 0, 1, 1, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index('Artist')\n",
    "\n",
    "# TODO : Print the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8WfvAvmNLp7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "# TODO : Extract users' likings (check loc method from df)\n",
    "user= ...\n",
    "other_line = ...\n",
    "\n",
    "# TODO : Extract all keys except the 'User' keys\n",
    "keys = [... for key in df.index ...]\n",
    "\n",
    "# TODO : Define a Similarity Function (Cosine or Distance will be enough)\n",
    "# Hint : Look at what we loaded for ya *wink wink*\n",
    "def similarity_function(l1, l2):\n",
    "  return ...\n",
    "\n",
    "# TODO : Compare each artists to the user's taste and print the list of recommendable artist.\n",
    "values = []\n",
    "for key in keys: \n",
    "  value = ...\n",
    "  values.append((key, value))\n",
    "  \n",
    "distance_df = pd.DataFrame(values, columns=[\"key\", \"similarity\"])\n",
    "distance_df.set_index('key')\n",
    "\n",
    "print(distance_df)\n",
    "\n",
    "# TODO : Conclude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMUa1ZCBn41I"
   },
   "source": [
    "# Hands-On II : Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr6WokIQQ739"
   },
   "source": [
    "## I - Load, Analyze, Transform Amazon Luxury\n",
    "\n",
    "As future Data Engineer, Data Analyst or Data Scientist, you will stumble upon the usual ETLs pipeline, that most companies lack in. As you might knowk, whether its for analysis or training, if garbage comes in, garbage goes out. So always, take time to correctly analyze your data.\n",
    "\n",
    "<img src=\"https://swordstem.com/wp-content/uploads/2019/04/372.png\">\n",
    "\n",
    "Useful doc: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAOEA4w8ShaC"
   },
   "source": [
    "\n",
    "We will be using an Industrial Dataset: Amazon Luxury. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKTnL5SmR_60"
   },
   "source": [
    "### a - Load\n",
    "\n",
    "Obviously, in this case the load step is not complex. But in your career, you will surely stumble on SQL, GraphQL, Neo4j, ElasticSearch.. databases, that are in an absolute mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6C7FdWmn9Gq"
   },
   "outputs": [],
   "source": [
    "# Loading /content/ratings_Beauty.csv into a dataframe\n",
    "\n",
    "df = pd.read_csv(\"/content/ratings_Beauty.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HijoVUk0SCa6"
   },
   "source": [
    "### b - Analyze\n",
    "\n",
    "\n",
    "The data is loaded into a Pandas Dataframe. Let's analyze and understand the dataset.\n",
    "\n",
    "\n",
    "* What are the Dataframes keys ?\n",
    "* How many items are there ? How many users are there ?\n",
    "* How many unique items are there ? How many unique users are there ?\n",
    "* What's the span of the ratings ?\n",
    "* Plot the number of Ratings that are present. Is it a Gaussian Distribution ? A Skewed Gaussian Distribution ? \n",
    "* What could it mean on the quality of the products ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-hEks0kqvAy"
   },
   "outputs": [],
   "source": [
    "# TODO : Perform the asked Data Exploration using pandas\n",
    "\n",
    "num_items = ...\n",
    "num_users = ...\n",
    "rating_span = ...\n",
    "\n",
    "# TODO : Plot the Distribution using plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGw52VdkoeXP"
   },
   "source": [
    "### c - Transform\n",
    "\n",
    "* Using pivot methods, transform your dataframe to a Rating Matrix. If your session crashes, pass this question. You'll create other Rating Matrix later.\n",
    "* Process the Nan Values of your Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmxA2pnHq88Z"
   },
   "outputs": [],
   "source": [
    "# We are limiting the dataset to the first 50 lines due to the limitation in Google Colab\n",
    "df_mf = df.head(50)\n",
    "\n",
    "# TODO : Create your rating matrix\n",
    "rating_matrix = ...\n",
    "\n",
    "# Print your rating matrix, what do you observe ?\n",
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpMqN7muuPJi"
   },
   "outputs": [],
   "source": [
    "# TODO : Replace all the Nan values with 0. Google what you need, the needed function is a method on Numpy\n",
    "def correct_outliers(R):\n",
    "    return ...\n",
    "\n",
    "rating_matrix_processed = correct_outliers(rating_matrix)\n",
    "# TODO : Plot the processed rating matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYzv3TXP4fgQ"
   },
   "source": [
    "## II - Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc1j53ZLoc_9"
   },
   "source": [
    "### a - Quick Theory Recap\n",
    "\n",
    "The good thing about Matrix Factorization is its usability for either content based recommendation or collaborative filtering based recommendation. This is one of the earliest model used by Netflix. \n",
    "\n",
    "Matrix Factorization finds latent structure in data such as it decomposes a Feedback Matrix $A\\in \\mathbb{R}^{M*N}$ with M the number of users and n the number of items, into :     \n",
    "* $P$ :  a user embedding matrix $\\in \\mathbb{R}^{M*K}$\n",
    "* $Q$ : an item embedding matrix $\\in \\mathbb{R}^{N*K}$\n",
    "\n",
    "The embeddings are learned such that \n",
    "$ \\hat{A} = P*Q^{T} \\approx A$\n",
    "\n",
    "We define the Objective Function as:\n",
    "<div align=center >\n",
    "$\\underset{P \\in \\mathbb{R}^{M \\times K}, Q \\in \\mathbb{R}^{N \\times K}}{\\text{minimize}}\\ \\underset{(i,j)\\in\\text{obs}}{\\sum} (A_{ij} - \\hat{A}_{ij})^2$\n",
    "over $P$ and  $Q$</div>\n",
    "\n",
    "To update the values of the matrixes, we use the typical Gradient Descent. In fact, each element of P and Q are updated using the following equations :\n",
    "\n",
    "$p_{ik} = p_{ik} + 2\\alpha*error* q_{kj}$\n",
    "\n",
    "$q_{kj} = q_{kj} + 2\\alpha*error* p_{ik}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mV3Fc3KWX0J"
   },
   "source": [
    "\n",
    "* Where do the $p_{ik}$ and $q_{kj}$ update formula come from ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICq7f2ziXfKz"
   },
   "source": [
    "### b - The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-znBCm4QWs5H"
   },
   "source": [
    "Now, we are going to code the Matrix Factorization Class that will perform matrix factorization for us.\n",
    "\n",
    "The class has 4 methods :\n",
    "* the init : for attribute initialization\n",
    "* the error : to compute the error\n",
    "* the update : to update the elements of P and Q \n",
    "* the run : to perform the calcultation for a num_step of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJQZrovMq4Y1"
   },
   "outputs": [],
   "source": [
    "class Matrix_Factorization():\n",
    "  \"\"\"\n",
    "  This class performs Matrix Factorization for a given FeedBack Matrix\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,A, latent_dim, steps= 5000, learning_rate = 0.0002):\n",
    "\n",
    "      self.A = A\n",
    "      self.latent_dim = latent_dim\n",
    "      self.steps = steps\n",
    "      self.learning_rate = learning_rate\n",
    "\n",
    "      # TODO : Initialize the correct attributes\n",
    "      # M,N are the shape of the FeedBack Matrix\n",
    "      self.M, self.N  = ...\n",
    "\n",
    "      # TODO : Initialize two random matrix P and Q. Don't forget to Transpose Q\n",
    "      self.P = ...\n",
    "      self.Q = ...\n",
    "\n",
    "  def error(self, a, b):\n",
    "      # TODO : Return the error : the difference between a and b\n",
    "      return ...\n",
    "\n",
    "  def update(self):\n",
    "    # TODO : Perform element update using gradient descent\n",
    "    # TODO : Iterate among the user\n",
    "    for user in range(...):\n",
    "        # TODO : Iterate among the items\n",
    "        for item in range(...):\n",
    "          if self.A[user][item]> 0:\n",
    "            # TODO : Compute the error between the GT Rating and the Predicted Rating of user on item\n",
    "            error = ...\n",
    "            # TODO : Using the gradient descent formula update the values of the elements of P and Q\n",
    "            for latent in range(self.latent_dim):\n",
    "                self.P[user][latent] = ...\n",
    "                self.Q[latent][item] = ...\n",
    "\n",
    "  \n",
    "  def run(self):   \n",
    "    # TODO : Call the update method 'steps' times   \n",
    "    for step in range(self.steps) : \n",
    "        ...\n",
    "    return np.dot(self.P, self.Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXfGbG-71nVC"
   },
   "source": [
    "### c - Toying Around : Testing it on Collaborative Filtering\n",
    "\n",
    "Before scaling up the model, let's first test it on a toy dataset. We give you this R matrix. As you can see the R matrix contains some outliers (Nan values)\n",
    "* How can we handle the outliers in this R matrix, what changes should you do ?\n",
    "* Define a function that handles the outliers of your input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsqeJlhp1lXP"
   },
   "outputs": [],
   "source": [
    "# Not TODO : Define a Rating Matrix. An example of Rating Matrix is given\n",
    "R = np.array([[np.NaN,5,1,0,4],\n",
    "              [0,0,1,0,3],\n",
    "              [0,5,0,2,np.NaN],\n",
    "              [1,np.NaN,0,5,1] ])\n",
    "\n",
    "# TODO : Define a function that handles the outliers of the R matrix. Hint nan_to_num \n",
    "def correct_outliers(R):\n",
    "    return ...\n",
    "\n",
    "# TODO : Verify that your function works\n",
    "R = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZE5Inac3Fae"
   },
   "outputs": [],
   "source": [
    "# TODO : Create an instance of the Matrix_Factorization Class\n",
    "latent_dim = ...\n",
    "MF = Matrix_Factorization(A = ...,\n",
    "                          latent_dim=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qP0MA4Dt5Hyh"
   },
   "outputs": [],
   "source": [
    "# TODO : Calculate the approximated Rating Matrix using the run method\n",
    "mat = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFEsDZN4Ch_T"
   },
   "source": [
    "### d - Scaling Up : Testing on the Luxury Dataset\n",
    "\n",
    "Now let's test our model on the Luxury Dataset. In order to check the capacity of the algorithm. We masked few ratings to Nan values. The goal is to check if the model would be able to recapture the original rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhqty0jDPN7a"
   },
   "source": [
    "The mf_df_train dataframe corresponds to the mf_df dataset but with some nan values in the Rating column. \n",
    "* Create a Rating Matrix using 'UserId' as Index and 'ProductId' as Columns.\n",
    "* Initialize well the values of the Rating Matrix and convert it to numpy array.\n",
    "* Run Matrix Factorization on the given Matrix for a feature space of size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfPpiYZiGCH0"
   },
   "outputs": [],
   "source": [
    "# TODO : Run Matrix Factorization for a latent space of dimension 2 \n",
    "latent_dim = ...\n",
    "MF = Matrix_Factorization(R,latent_dim)\n",
    "R_pred = MF.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO76Gn7hQjY0"
   },
   "source": [
    "Now let's answer the following questions : \n",
    "* Will user 6 like item 9 ?\n",
    "* Will user 4 like item 2 ?\n",
    "* What do you think of this model ? (Is it fast ? Is it efficient ? Does it need lot of engineering ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSsUwP-oYO6g"
   },
   "source": [
    "# Hands-On III : Introducing Lightning with Recommendation Systems\n",
    "\n",
    "Using MovieLens Dataset, build a recommendation system. Feel free for your model but we highly, suggest you to inspire yourself from the NCF Model. This part is **voluntarily sparse**. As future engineers, you'll need to go fetch informations everywhere in order to create and train your model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzts_juFeAgM"
   },
   "source": [
    "## I - Load, Analyze and Transform MovieLens Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ow472CxeEez"
   },
   "source": [
    "### a - Just do It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-HveM9ZYieW"
   },
   "source": [
    "Using Panda Dataframe have a look at the ratings.csv and movies.csv files and answer the following questions.\n",
    "\n",
    "* What are the features of both csv files ?\n",
    "* To you, what could be the most important csv file ?\n",
    "* How many movies are there ?\n",
    "* How many users rated movies ? \n",
    "* How users did rate the movies ? \n",
    "* What's the span of the ratings ? Using matplotlib or seaborn, plot the ratings for a given movie.\n",
    "* Can we merge both tables ? if yes what key should be used to merge them ?\n",
    "* What are the differents genres of the movies ?\n",
    "* Are there outliers ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVIq4szvaj76"
   },
   "outputs": [],
   "source": [
    "# TODO : Perform your Exploratory Data Analysis\n",
    "ratings = \n",
    "movies = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hg8zt9ea1_u"
   },
   "source": [
    "### b - Some Changes\n",
    "\n",
    "For the rest of the lab, we will only use .05% of the dataset as it it too much for this notebook ( RIP Google ). Run the following cell for the rating dataframe. It will replace the existing rating dataset by a splitted dataset corresponding to 0.05% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVkJ5S1LbMLq"
   },
   "outputs": [],
   "source": [
    "ratings, _ = train_test_split(ratings, test_size = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlIXMBoRbc1Q"
   },
   "source": [
    "### c - Create your Dataset class\n",
    "At this point, you should have a good understanding of the given dataset.As we work with Tensors and Pytorch, we need to convert the Dataset to something readable by a Pytorch Model. Define a Dataset class that inherits from Dataset that returns a dictionnary {user, item, ratings}. The skeleton is given.\n",
    "Don't forget to create your train, test, validation datasets.\n",
    "\n",
    "Documentation available on : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YW4LI87cAdy"
   },
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "\n",
    "  def __init__(self,dataframe):\n",
    "      self.dataframe = \n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "\n",
    "      return {'user': torch.tensor(, dtype=torch.long),\n",
    "              'movie':  torch.tensor(, dtype=torch.long),\n",
    "              'rating':  torch.tensor(, dtype=torch.float)}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gi20sLNbfbt"
   },
   "source": [
    "### d - Create your DataLoaders\n",
    "\n",
    "Now as in the labs, we want to fetch lot of data at the same time to send it to the model. We use DataLoaders. Define Dataloaders for each of your DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYvkMJCwcsot"
   },
   "outputs": [],
   "source": [
    "dataloader_train = ...\n",
    "dataloader_valid = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P3EpMnMbiwZ"
   },
   "source": [
    "## II - Create a Model \n",
    "\n",
    "Create a model that inherits from pl.Lightning. On the contrairy of PyTorch nn.Module, a Lihgnting Model encompasses the training, validation steps. A skeleton is given. If you don't have any ideas, try to reimplement NCF model.\n",
    "\n",
    "Documentation available on : https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkCM-u6KcrQi"
   },
   "outputs": [],
   "source": [
    "class NCF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        # TODO : Create your model\n",
    "\n",
    "    def forward(self, ...):\n",
    "        # TODO : Send what you need in your model\n",
    "\n",
    "        return ...\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # TODO : Define your training step\n",
    "        user_ids, item_ids, ratings = ...\n",
    "\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       # TODO : Define your validation step, don't forget to compute your accuracy\n",
    "        user_ids, item_ids, ratings = ...\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log('val_accuracy', self.accuracy.compute())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Et8PBiygYyQ"
   },
   "source": [
    "## III - Trainer\n",
    "\n",
    "Obviously, you can rewrite the same writing loop everytime, but we can be smarter and create a Trainer, that already has everything written in it. \n",
    "\n",
    "More Documentations on: https://lightning.ai/docs/pytorch/stable/common/trainer.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCFW5CMUjvRx"
   },
   "source": [
    "### a - Set Up Tensorboard for Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_r5-eF3Ij2fJ"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2CuIhEzkfHP"
   },
   "source": [
    "### b - Create your trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiQ_-Ff3kiWX"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=-1, \n",
    "                     max_epochs=..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlOXFaW5bnyx"
   },
   "source": [
    "### c -  Train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH61ZalJcuXS"
   },
   "outputs": [],
   "source": [
    "trainer.fit(..., \n",
    "            ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2ID6GmcbpR2"
   },
   "source": [
    "### d - Test\n",
    "\n",
    "Test your model on the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZgUaa7Ycu0A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPyxs8hJT7Q9kHC4afuLNEF",
   "include_colab_link": true,
   "name": "RecSysTD.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
