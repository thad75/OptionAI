{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/OptionAI/blob/main/TP3/TP3_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3, Part 2 : Feed Forward Neural Language Models"
      ],
      "metadata": {
        "id": "KaMsBkHIS_fY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJMQLZ_uf6tI"
      },
      "source": [
        "# About this lab\n",
        "\n",
        "In this session, you will experiment with feed-forward neural language models (FFLM) using [PyTorch](https://www.pytorch.org). To train the models, you will be using the [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) corpus, which is a popular LM dataset introduced in 2016:\n",
        "\n",
        "> The WikiText language modeling dataset is a collection of texts extracted from Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), `WikiText-2` is over 2 times larger. The dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\n",
        "\n",
        "Goal of this lab : \n",
        "* Understand FFN\n",
        "* Train a FFNLM\n",
        "* Use PyTorch\n",
        "\n",
        "This part should take you 3h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Stuff & Setting Up the Environment"
      ],
      "metadata": {
        "id": "k5w6kOKHTYfB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eLd2J2B1i2u"
      },
      "source": [
        "# Download the corpus\n",
        "%%bash\n",
        "URL=\"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
        "\n",
        "for split in \"train\" \"valid\" \"test\"; do\n",
        "  if [ ! -f \"${split}.txt\" ]; then\n",
        "    echo \"Downloading ${split}.txt\"\n",
        "    wget -q \"${URL}/${split}.txt\"\n",
        "    # Remove empty lines\n",
        "    sed -i '/^ *$/d' \"${split}.txt\"\n",
        "    # Remove article titles starting with = and ending with =\n",
        "    sed -i '/^ *= .* = $/d' \"${split}\".txt\n",
        "  fi\n",
        "done\n",
        "\n",
        "# Prepare smaller version for fast training neural LMs\n",
        "head -n 5000 < train.txt > train_small.txt\n",
        "\n",
        "# Print the first 10 lines with line numbers\n",
        "cat -n train.txt | head -n10\n",
        "echo\n",
        "\n",
        "# Print some statistics\n",
        "echo -e \"\\n   Line,   word,   character counts\"\n",
        "wc *.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to allow deterministic behaviour, that is, make results reproducible\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "metadata": {
        "id": "ZFy3NcUHP7Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFcmlFdf6tK"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "# Fancy progress bar\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "###############\n",
        "# Torch setup #\n",
        "###############\n",
        "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
        "cuda_available = torch.cuda.is_available()\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for Neural LM experiments!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'\n",
        "\n",
        "#######################\n",
        "# Some helper functions\n",
        "#######################\n",
        "def fix_seed(seed=None):\n",
        "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  if seed is None:\n",
        "    # Take a random seed\n",
        "    seed = time.time()\n",
        "  seed = int(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  return seed\n",
        "\n",
        "def readable_size(n):\n",
        "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
        "  sizes = ['K', 'M', 'G']\n",
        "  fmt = ''\n",
        "  size = n\n",
        "  for i, s in enumerate(sizes):\n",
        "    nn = n / (1000 ** (i + 1))\n",
        "    if nn >= 1:\n",
        "      size = nn\n",
        "      fmt = sizes[i]\n",
        "    else:\n",
        "      break\n",
        "  return '%.2f%s' % (size, fmt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmP0ZWbvB1aa"
      },
      "source": [
        "# Feed-forward Language Models (FFLM)\n",
        "\n",
        "FFLMs are similar to $n$-gram language models in the sense that the choice of $n$ is a hyperparameter for the network architecture. A basic FFLM constructs a  $C=n\\mathrm{-1}$ length context window before the word to be predicted. If the word embedding size is $E$, the feature vector for the context window becomes a vector of size $E\\times C$, resulting from the **concatenation** of individual word embeddings of context words. Hence, the choice of $C$ for FFLMs, affects the number of final learnable parameters in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A - Dataset Stuff"
      ],
      "metadata": {
        "id": "c_sCtdDlUIvV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWMhsDZOf6tS"
      },
      "source": [
        "### Representing the vocabulary\n",
        "\n",
        "The below `Vocabulary` class encapsulates the **word-to-idx** and **idx-to-word** mapping that you should now be familiar with from the previous lab sessions. Read it to understand how the vocabulary is constructed from a plain text file, within the `build_from_file()` method. Special `<.>` markers are also included in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlxMOBzPf6tT"
      },
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
        "  def __init__(self):\n",
        "    # Mapping from tokens to integers\n",
        "    self._word2idx = {}\n",
        "\n",
        "    # Reverse-mapping from integers to tokens\n",
        "    self.idx2word = []\n",
        "\n",
        "    # 0-padding token\n",
        "    self.add_word('<pad>')\n",
        "    # sentence start\n",
        "    self.add_word('<s>')\n",
        "    # sentence end\n",
        "    self.add_word('</s>')\n",
        "    # Unknown words\n",
        "    self.add_word('<unk>')\n",
        "\n",
        "    self._unk_idx = self._word2idx['<unk>']\n",
        "\n",
        "  def word2idx(self, word):\n",
        "    \"\"\"Returns the integer ID of the word or <unk> if not found.\"\"\"\n",
        "    return self._word2idx.get(word, self._unk_idx)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\n",
        "    if word not in self._word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\n",
        "\n",
        "  def build_from_file(self, fname):\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\n",
        "    with open(fname) as f:\n",
        "      for line in f:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "          self.add_word(word)\n",
        "\n",
        "  def convert_idxs_to_words(self, idxs):\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
        "\n",
        "  def convert_words_to_idxs(self, words):\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\n",
        "    return [self.word2idx(w) for w in words]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hE1Lr6j_oYB"
      },
      "source": [
        "Let's construct the vocabulary for the training set and analyse the token indices for a sentence with an unknown word.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Why do we map unknown tokens to a special `<unk>` token?**\n",
        "* **Do you think the network will learn a useful embedding for that? If not, how can you let the network to learn an embedding for it?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcNMhXqB5wwT"
      },
      "source": [
        "vocab = Vocabulary()\n",
        "vocab.build_from_file('train.txt')\n",
        "print(vocab)\n",
        "\n",
        "# TODO : Convert sentence to list of indices, note how the last word is mapped to 3 (<unk>)\n",
        "print(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-4PQ9_8f6tV"
      },
      "source": [
        "### Representing the corpus\n",
        "\n",
        "Let's process the corpus for PyTorch: all splits will end up being a large, 1D token sequences. Note that, in `corpus_to_tensor()`, every line is wrapped between `<s> .. </s>` tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5s9MMt06YsL"
      },
      "source": [
        "def corpus_to_tensor(_vocab, filename):\n",
        "  # Final token indices\n",
        "  idxs = []  \n",
        "  with open(filename) as data:\n",
        "    for line in tqdm(data, ncols=80, unit=' line', desc=f'Reading {filename} '):\n",
        "      line = line.strip()\n",
        "      # Skip empty lines if any\n",
        "      if line:\n",
        "        # Each line is considered as a long sentence for WikiText-2\n",
        "        line = f\"<s> {line} </s>\"\n",
        "        # Split from whitespace and add sentence markers\n",
        "        idxs.extend(_vocab.convert_words_to_idxs(line.split()))\n",
        "  return torch.LongTensor(idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl6xuwZS9uO1"
      },
      "source": [
        "# Read the files, prepare the small one as well\n",
        "train = corpus_to_tensor(vocab, 'train.txt')\n",
        "train_small = corpus_to_tensor(vocab, 'train_small.txt')\n",
        "\n",
        "valid = corpus_to_tensor(vocab, 'valid.txt')\n",
        "test = corpus_to_tensor(vocab, 'test.txt')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Small training size in tokens: {readable_size(len(train_small))}')\n",
        "print(f'Training size in tokens: {readable_size(len(train))}')\n",
        "print(f'Validation size in tokens: {readable_size(len(valid))}')\n",
        "print(f'Test size in tokens: {readable_size(len(test))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNxNrdhPECML"
      },
      "source": [
        "**Q: Print the first 20 token indices from the training set. And then print the sentence in actual words corresponding to these 20 tokens by using one of the provided methods in the `Vocabulary` class.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXobO9_n-Giz"
      },
      "source": [
        "########\n",
        "# Answer\n",
        "########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiroR-NszhGY"
      },
      "source": [
        "## B - Model definition\n",
        "\n",
        "Now that we are done with data loading and vocabulary construction, we can define the actual FFLM model in PyTorch. Recall from the lectures that this model requires a pre-defined context window size $C$ which will affect the way you set up some of the linear layers. **Note that**, in contrast to the model depicted in the lecture, this model has an additional layer `ff_ctx`, which projects the context vector $c_k$ to hidden dimension $H$. This ensures that the number of parameters in the output layer does not depend on the context size, i.e. it is always $H\\times V$ instead of $CE\\times V$.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Follow the comments in `__init__()` and `forward()` to fill in the missing parts with some actual code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ttYW2IC_UV"
      },
      "source": [
        "class FFLM(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, context_size, dropout=0.5):\n",
        "    # Call parent's __init__ first\n",
        "    super(FFLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.context_size = context_size\n",
        "\n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Create the non-linearity\n",
        "    self.nonlin = torch.nn.Tanh()\n",
        "\n",
        "    # Dropout regularizer\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    ##############################\n",
        "    # Fill the missing parts below\n",
        "    ##############################\n",
        "    # TODO : Compute the dimension of the context vector\n",
        "    self.context_dim = ...\n",
        "    \n",
        "    # Create the embedding layer (i.e. lookup table tokens->vectors)\n",
        "    self.emb = nn.Embedding(\n",
        "        num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "        padding_idx=0)\n",
        " \n",
        "    # This cuts the number of parameters a bit\n",
        "    self.ff_ctx = nn.Linear(self.context_dim, self.hid_dim)\n",
        "\n",
        "    ############################################\n",
        "    # Output layer mapping from the output of `ff_ctx` to vocabulary size\n",
        "    # TODO : Fill the dimensions of the output layer\n",
        "    ############################################\n",
        "    self.out = nn.Linear(...)\n",
        "\n",
        "    # Purely for informational purposes: compute # of total params\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "        self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "      \n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # TODO : What is the shape of x ?\n",
        "    ...\n",
        "    # TODO : Get the embeddings for the token indices in `x`\n",
        "    embs = ...\n",
        "    # TODO : Concatenate the embeddings to form the context vector\n",
        "    ctx = ...\n",
        "\n",
        "    # TODO : Apply ff_ctx -> non-lin -> dropout -> output layer to obtain the logits i.e. unnormalized scores   \n",
        "    logits = ...\n",
        "\n",
        "    # TODO : Use self.loss to compute the losses, return the losses (true labels are in `y`)\n",
        "    return ...\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size=64):\n",
        "    \"\"\"Returns a tensor of size (n_batches, batch_size, context_size + 1).\"\"\"\n",
        "    # Split data into rows of n-grams followed by the (n+1)th true label\n",
        "    x_y = data_tensor.unfold(0, self.context_size + 1, step=1)\n",
        "\n",
        "    # Get the number of training n-grams\n",
        "    n_samples = x_y.size()[0]\n",
        "\n",
        "    # Hack: discard the last uneven batch for simplicity\n",
        "    n_batches = n_samples // batch_size\n",
        "    n_samples = n_batches * batch_size\n",
        "    # Split nicely into batches, i.e. (n_batches, batch_size, context_size + 1)\n",
        "    # The final element in each row is the ID of the true label to predict\n",
        "    x_y = x_y[:n_samples].view(n_batches, batch_size, -1)\n",
        "\n",
        "    # A particular batch for context_size=2 will now look like below in\n",
        "    # word format. Last element for every array is the next token to be predicted\n",
        "    #\n",
        "    # [[<s>, cat, sat],\n",
        "    #  [cat, sat, on],\n",
        "    #  [sat, on,  the],\n",
        "    #  [on,  the, mat],\n",
        "    #   ....\n",
        "    return x_y\n",
        "\n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64, shuffle=False):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for the training data\n",
        "    batches = self.get_batches(train_tensor, batch_size)\n",
        "    \n",
        "    print(f'Will do {batches.size(0)} batches for an epoch.')\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Shuffle the batch order or not\n",
        "      if shuffle:\n",
        "        batch_order = torch.randperm(batches.size(0))\n",
        "      else:\n",
        "        batch_order = torch.arange(batches.size(0))\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, idx in enumerate(batch_order):\n",
        "        batch = batches[idx].to(DEVICE)\n",
        "\n",
        "        # TODO : Split into inputs `x` and labels `y`. Hint : Look at the context_size\n",
        "        x, y = ...,...\n",
        "\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # TODO : Compute the loss thanks to one of the previous function\n",
        "        loss = ...\n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 1000 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          ppl = math.exp(loss_per_token)\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss, valid_ppl = self.evaluate(test_set=valid_tensor)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss, test_ppl = self.evaluate(test_set=test_tensor)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
        "\n",
        "  def evaluate(self, test_set, batch_size=32):\n",
        "    \"\"\"Evaluates and computes perplexity for the given test set.\"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    # Get the batches\n",
        "    batches = self.get_batches(test_set, batch_size)\n",
        "\n",
        "    # TODO : Set your model to Eval mode\n",
        "    self. ....\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in batches:\n",
        "        batch = batch.to(DEVICE)\n",
        "\n",
        "        # TODO : Split into inputs `x` and labels `y`. Hint : Look at the context_size\n",
        "        x, y = ...,...\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        loss += self.forward(x, y).sum()\n",
        "    \n",
        "    # Normalize by the number of tokens in the test set\n",
        "    loss /= batches.size()[:2].numel()\n",
        "\n",
        "    # TODO : Switch back to training mode\n",
        "    self. ...\n",
        "\n",
        "    # return the perplexity and loss\n",
        "    return loss, math.exp(loss)\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(FFLM, self).__repr__()\n",
        "    return f\"{s}\\n# of parameters: {self.n_params}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0eHpYiRUHDe"
      },
      "source": [
        "## C - Training\n",
        "\n",
        "We can now launch training using a set of sane hyper-parameters for our model. This is a 3-gram FFLM since the context size is set to 2. On a Colab GPU, a single epoch should take around 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rjwvEYYFjjE"
      },
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(42)\n",
        "\n",
        "fflm_model = FFLM(\n",
        "    len(vocab),       # vocabulary size\n",
        "    emb_dim=128,      # word embedding dim\n",
        "    hid_dim=128,      # hidden layer dim\n",
        "    context_size=2,   # C = (N-1) if you think in n-gram LM terminology\n",
        "    dropout=0.4,      # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "fflm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "FFLM_INIT_LR = 0.001\n",
        "\n",
        "# Create the optimizer\n",
        "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "print(fflm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "# It will print progress every 1000 batches\n",
        "fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAHN-C-XHuVs"
      },
      "source": [
        "**Q: If everything goes well, you should see a loss of around ~10.4 printed as the first loss. This will still be the case if you change the random seed to some other number before model construction i.e. the culprit is not the exact values that they take.**\n",
        "* **Can you come up with a simple mathematical formula which yields that value?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiG_vI1uJj1X"
      },
      "source": [
        "##########################\n",
        "# Answer to question above\n",
        "##########################\n",
        "print(\"<TODO: put the formula here which computes the value>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsYXRU68SWF7"
      },
      "source": [
        "## D - Further Exploring\n",
        "\n",
        "With the default settings above, you should end up with a validation perplexity of $\\sim1076$ and a final test set perplexity of $\\sim1003$ at the end of 5th epoch. Try the following questions to further analyze the model's prediction.\n",
        "\n",
        "---\n",
        "\n",
        "* **Q: Remove the `tanh()` non-linearity from the code so that the context is computed as a linear combination of its embeddings. How does the results compare to the initial one? Do you think non-linearity helps?**\n",
        "\n",
        "* **Q: Compare the results by rerunning the training with unshuffled batches i.e. with `shuffle=False`. What do you notice in terms of results?**\n",
        "\n",
        "* **Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
        "\n",
        "* **Q: Try with different context sizes such as 3, 5, 7, etc. What is the best perplexity you can get?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq1fn-x9aS5e"
      },
      "source": [
        "## E - Further Reading for your knowledge\n",
        " - [Original FFLM paper from Bengio et al. 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        " - [Original RNNLM paper from Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
        " - Some recent state-of-the-art LSTM-based RNNLMs\n",
        "\n",
        "  - [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)\n",
        "  - [An Analysis of Neural Language Modeling at Multiple Scales](https://arxiv.org/pdf/1803.08240.pdf)\n",
        "  - [Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours](https://mlsys.org/Conferences/2019/doc/2018/50.pdf)"
      ]
    }
  ]
}